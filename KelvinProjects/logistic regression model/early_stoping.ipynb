{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b311fae-cefd-4b12-95ea-02737b140453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc88641-3052-4793-936c-94ca77e03669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "484cda1b-2d3d-47db-80f7-d90d976256bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7       8   \\\n",
       "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "\n",
       "        9   ...     22     23      24      25      26      27      28      29  \\\n",
       "0  0.14710  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       30       31  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6172dfb9-4a43-4251-b93d-799c2e3e5a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12425162-db0e-41f1-a1cb-9d4f16d6bc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kelvi\\AppData\\Local\\Temp\\ipykernel_22972\\417157543.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[1]=df[1].replace({'M':1,'B':0})\n"
     ]
    }
   ],
   "source": [
    "df[1]=df[1].replace({'M':1,'B':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf34733-cbc3-4ed9-aeaf-caef57cff537",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=df.iloc[0:300]\n",
    "val_set=df.iloc[300:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd575be-1aad-47e2-bdbd-8ea15f28c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_set[[2,3,4,5,6,7,8,9,10]]\n",
    "y=val_set[1]\n",
    "X_val=val_set[[2,3,4,5,6,7,8,9,10]]\n",
    "y_val=val_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a124f607-cdb5-487c-bdef-63f6824a1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class early_stoping:\n",
    "    def __init__(self,batch_size=30,lr=0.001,epochs=1000,patience=5,delta=0.001):\n",
    "        self.batch_size=batch_size\n",
    "        self.lr=lr\n",
    "        self.epochs=epochs\n",
    "        self.weight=None\n",
    "        self.bias=None\n",
    "        self.loss_history=[]\n",
    "        self.wait=0\n",
    "        self.patience=patience\n",
    "        self.delta=delta\n",
    "        self.best_weight_v_set=None\n",
    "        self.best_bias_v_set=None\n",
    "        self.best_loss=None\n",
    "        self.val_loss_history=[]\n",
    "        self.early_stop=False\n",
    "\n",
    "    def log_odds(self,X):\n",
    "        return np.dot(self.weight,X) + self.bias\n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def train(self,X,y,X_val,y_val):\n",
    "        n_sample,n_features=X.shape\n",
    "        np.random.seed(42)\n",
    "        self.weight=np.random.randn(X.shape[1])\n",
    "        self.bias=np.random.randn()\n",
    "        for i in range(self.epochs):\n",
    "            indeces=np.random.permutation(n_sample)\n",
    "            X=X.to_numpy()\n",
    "            y=y.to_numpy()\n",
    "            X_shuffle=X[indeces]\n",
    "            y_shuffle=y[indeces]\n",
    "            for j in range(0,batch_n_sample,self.batch_size):\n",
    "                X_batch=X_shuffle[j:j+self.batch_size]\n",
    "                y_batch=y_shuffle[j:j+self.batch_size]\n",
    "                batch_size_current= len(X_batch)\n",
    "                z=self.log_odds(X_batch)\n",
    "                predict=self.sigmoid(z)\n",
    "                error=predict - y_batch\n",
    "\n",
    "                gradient_weight=(1/batch_size_current)*np.dot(error,X_batch.T)\n",
    "                gradient_bias=(1/batch_size_current)*np.sum(error)\n",
    "                \n",
    "                self.weight -= lr * gradient_weight\n",
    "                self.bias -= lr * gradient_bias\n",
    "\n",
    "                loss= -np.mean(y_batch * np.log(predict + 1e-15) + (1 - y_batch)*np.log(1 - predict + 1e-15))\n",
    "                self.loss_history.append(loss)\n",
    "\n",
    "                z_val_set= np.dot(X_val.T,self.weight) + self.bias\n",
    "                pred_val_set= 1 /(1 + np.exp(-z_val_set))\n",
    "\n",
    "            val_error=pred_val_set - y_val\n",
    "            val_loss= -np.mean(y_val * np.log(pred_val_set) + (1 - y_val + 1e-15)*np.log(1 - pred_val_set + 1e-15))\n",
    "            self.val_loss_history.append(val_loss)\n",
    "\n",
    "            print(f' the self.val_loss_history is {self.val_loss_history}')\n",
    "            if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
    "                self.best_loss = val_loss\n",
    "                self.best_weight_v_set=np.copy(self.weight)\n",
    "                self.best_bias_v_set=np.copy(self.bias)\n",
    "                self.wait=0\n",
    "            else:\n",
    "                self.wait= +1\n",
    "                if self.wait >= self.patience:\n",
    "                    print(f' early stoping is at {i} epochs')\n",
    "                    self.early_stop=True\n",
    "                    break\n",
    "        if self.best_weight_v_set is not None:\n",
    "            self.best_weight_v_set=self.weight\n",
    "            self.best_bias_v_set=self.bias\n",
    "                \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4da849-4f07-46e2-bdaf-18b425c406a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=early_stoping()\n",
    "d.train(X,y,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b936dcec-1c90-4d69-833b-2acc6c0dda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic:\n",
    "    def __init__(self,patience=6,epochs=1000,batchsize=30,delta=0.01,lr=0.01):\n",
    "        self.patience=patience\n",
    "        self.epochs=epochs\n",
    "        self.batchsize=batchsize\n",
    "        self.delta=delta\n",
    "        self.lr=lr\n",
    "        self.weight=None\n",
    "        self.bias=None\n",
    "        self.best_weight=None\n",
    "        self.best_bias=None\n",
    "        self.best_loss=None\n",
    "        self.val_loss_history=[]\n",
    "        self.train_loss_hist=[]\n",
    "        self.wait=0\n",
    "        self.early_stop=False\n",
    "    def sigmoid(self,z):\n",
    "        z=np.clip(z,-500,500)\n",
    "        return 1/(1+np.exp(-z))\n",
    "    def train(self,X,y,X_val,y_val):\n",
    "        n_sample_train,n_feature_train=X.shape\n",
    "        # n_sample_val,n_feature_val=X_val.shape\n",
    "\n",
    "        np.random.seed(42)\n",
    "        self.weight=np.random.randn(n_feature_train)\n",
    "        self.bias=np.random.randn()\n",
    "        X=(X-X.mean())/X.std()+1e-8\n",
    "        X_val=(X_val-X_val.mean())/X_val.std()+1e-8\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            indices=np.random.permutation(len(y))\n",
    "            X=X.to_numpy()\n",
    "            y=y.to_numpy()\n",
    "            X_shaffle=X[indices]\n",
    "            y_shaffle=y[indices]\n",
    "            for _ in range(0,len(y),self.batchsize):\n",
    "                X_batch=X_shaffle[i:i+self.batchsize]\n",
    "                y_batch=y_shaffle[i:i+self.batchsize]\n",
    "                n_sample_batch=len(X_batch)\n",
    "\n",
    "                z=np.dot(X_batch,self.weight)+ self.bias\n",
    "                y_predi=self.sigmoid(z)\n",
    "                error=y_predi-y_batch\n",
    "\n",
    "                gradient_weight =(1/n_sample_batch)*np.dot(X_batch.T,error)\n",
    "                gradient_bias  =(1/n_sample_batch) *np.sum(error)\n",
    "\n",
    "                self.weight -= self.lr*gradient_weight\n",
    "                self.bias -= self.lr*gradient_bias\n",
    "\n",
    "                loss_train =-np.mean(y_batch*np.log(y_predi +1e-15) +(1-y_batch)*np.log(1-y_predi +1e-15))\n",
    "                self.train_loss_hist.append(loss_train)\n",
    "\n",
    "            z1=np.dot(X_val,self.weight) +self.bias\n",
    "            y_predi_val=self.sigmoid(z1)\n",
    "            loss_val = -np.mean(y_val*np.log(y_predi_val) +(1-y_val +1e-15)*np.log(1-y_predi_val +1e-15))\n",
    "            self.val_loss_history.append(loss_val)\n",
    "\n",
    "            if self.best_loss is None or loss_val < self.best_loss -self.delta:\n",
    "                self.best_loss=loss_val\n",
    "                self.best_weight =np.copy(self.weight)\n",
    "                self.best_bias =np.copy(self.bias)\n",
    "                self.wait=0\n",
    "            else:\n",
    "                self.wait +=1\n",
    "                if self.wait >=self.patience:\n",
    "                    print(f\"early stopping at epoch {i}\")\n",
    "                    self.early_stop=True\n",
    "                    break\n",
    "        if self.best_weight is not  None:\n",
    "            self.weight=self.best_weight\n",
    "            self.bias=self.best_bias\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5273e194-44d8-4d44-b353-5abf878e81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=logistic()\n",
    "r.train(X,y,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3b655-725a-4c9f-a39e-aae27eb1095e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
