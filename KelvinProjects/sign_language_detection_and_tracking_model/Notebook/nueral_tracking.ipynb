{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef00b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "71262348",
   "metadata": {},
   "outputs": [],
   "source": [
    "image='data/images'\n",
    "Annotation='data/Annotations'\n",
    "classes={'okay':0,\"hello\":1,'thankyou':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "83db1472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9c4352be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_voc_annotations(Annotation,image,classes):\n",
    "    img_path=[]\n",
    "    class_label=[]\n",
    "    bbox=[]\n",
    "    for xml_file in sorted(os.listdir(Annotation)):\n",
    "        if not xml_file.endswith('.xml'):\n",
    "            continue\n",
    "            # print(xml_file)\n",
    "        tree=ET.parse(os.path.join(Annotation,xml_file))\n",
    "        print(tree)\n",
    "        root=tree.getroot()\n",
    "        image_file_name=root.find('filename').text\n",
    "        path=os.path.join(image,image_file_name)\n",
    "        size=root.find('size')\n",
    "\n",
    "        image_width=int(size.find('width').text)\n",
    "        image_height=int(size.find('height').text)\n",
    "\n",
    "        objects=root.find('object')\n",
    "        if object is not None:\n",
    "            class_name=objects.find('name').text\n",
    "            if class_name not in classes:\n",
    "                continue\n",
    "            class_id=classes[class_name]\n",
    "\n",
    "            bnbox=objects.find('bndbox')\n",
    "            xmin=float(bnbox.find('xmin').text)/image_width\n",
    "            ymin=float(bnbox.find('ymin').text)/image_height\n",
    "            xmax=float(bnbox.find('xmax').text)/image_width\n",
    "            ymax=float(bnbox.find('ymax').text)/image_height\n",
    "\n",
    "            img_path.append(path)\n",
    "            bbox.append([xmin,ymin,xmax,ymax])\n",
    "            class_label.append(class_id)\n",
    "    return img_path,bbox,class_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab899005",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path,bnbox_data,class_labels=parse_voc_annotations(Annotation=Annotation,image=image,classes=classes)\n",
    "img_path=tf.constant(img_path)\n",
    "bnbox_data=tf.constant(bnbox_data,dtype=tf.float32)\n",
    "class_labels=tf.constant(class_labels,dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82aa540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path, bbox, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image.set_shape([None, None, 3])\n",
    "    image = tf.image.resize(image, [128, 128])\n",
    "    label_encode = tf.one_hot(label, depth=len(classes))\n",
    "    image = image / 255.0\n",
    "    return image, {\"class_output\": label_encode, \"box_output\": bbox}\n",
    "\n",
    "dataset= tf.data.Dataset.from_tensor_slices((img_path, bnbox_data, class_labels))\n",
    "dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=71).batch(10).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "DATASET_SIZE = len(img_path)\n",
    "train_size = int(0.8 * DATASET_SIZE)\n",
    "train_ds = dataset.take(train_size)\n",
    "val_ds = dataset.take(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b05b0cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d004ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"img_path length: {len(img_path)}\")\n",
    "print(f\"bnbox_data length: {len(bnbox_data)}\")\n",
    "print(f\"class_labels length: {len(class_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9d6d9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(128,128,3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d22a1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "input=tf.keras.layers.Input(shape=(128,128,3))\n",
    "\n",
    "x=tf.keras.layers.RandomZoom(0.1)(input)\n",
    "x=tf.keras.layers.RandomBrightness(0.2)(x)\n",
    "x=tf.keras.layers.RandomFlip('vertical and horizontal')(x)\n",
    "x=tf.keras.layers.Rescaling(1./255)(x)\n",
    "x=base_model(input)\n",
    "\n",
    "x=tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "\n",
    "class_output=tf.keras.layers.Dense(30,activation='relu')(x)\n",
    "class_output=tf.keras.layers.Dense(3,activation='softmax',name='class_output')(class_output)\n",
    "\n",
    "box_output=tf.keras.layers.Dense(30,activation='relu')(x)\n",
    "box_output=tf.keras.layers.Dense(4,activation='sigmoid',name='box_output')(box_output)\n",
    "\n",
    "detecting_signs_model=tf.keras.models.Model(inputs=input,outputs=[class_output,box_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "47c07fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "detecting_signs_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        \n",
    "        'class_output':'categorical_crossentropy',\n",
    "        'box_output':'mse'\n",
    "    },\n",
    "    metrics={\n",
    "        'class_output':'accuracy',\n",
    "        'box_output':'mse'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89267e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detecting_signs_model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_class_output_accuracy',patience=3,restore_best_weights=True,mode='max'),\n",
    "        tf.keras.callbacks.ModelCheckpoint('sign_language.keras',monitor='val_class_output_loss',save_best_only=True,mode='min',verbose=1)\n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f332eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n"
     ]
    }
   ],
   "source": [
    "image=tf.keras.utils.load_img('data/images/0f3ccdf7-IMG-20250715-WA0040.jpg',target_size=(128,128))\n",
    "img_array=tf.keras.utils.img_to_array(image)\n",
    "norm=img_array/.255\n",
    "img_batch=np.expand_dims(norm,axis=0)\n",
    "too=inference.predict(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cbc6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['okay',\"hello\",'thankyou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c23f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes=[np.argmax(s) for s in too]\n",
    "predicted=predicted_classes[0]\n",
    "classes[predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09b71881",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference=tf.keras.models.load_model('sign_language.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4edcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise Exception('could not open the camera')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    original = frame.copy() \n",
    "\n",
    "    frame = cv2.resize(frame, (128, 128))       \n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  \n",
    "    input_data = frame_rgb / 255.0               \n",
    "    input_data = input_data.reshape(1, 128, 128, 3) \n",
    "\n",
    "    predictions = inference.predict(input_data) \n",
    "    Prediction= [np.argmax(p) for p in predictions]\n",
    "    s=Prediction[0]\n",
    "    print(classes[s])\n",
    "\n",
    "\n",
    "    cv2.putText(original, f'Prediction: {classes[s]}',\n",
    "                \n",
    "                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    try:\n",
    "        x, y, w = map(int, predictions[0].flatten())\n",
    "    \n",
    "        scale_x = original.shape[1] / 128\n",
    "        scale_y = original.shape[0] / 128\n",
    "        x = int(x * scale_x)\n",
    "        y = int(y * scale_y)\n",
    "        w = int(w * ((scale_x + scale_y) / 2))  \n",
    "        cv2.circle(original, (x, y), w, (0, 255, 0), 2)\n",
    "    except Exception as e:\n",
    "        print(\"Prediction output error:\", e)\n",
    "\n",
    "    cv2.imshow('Real-Time Inference', original)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4961962",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f268ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
